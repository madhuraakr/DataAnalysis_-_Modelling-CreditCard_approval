# -*- coding: utf-8 -*-
"""DimensionalityReduction, Modelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CQqjJt-3-ozxr7ImwrUO-2Li59cEKjhG

### Non parametric approach
31-01-2021
###  PCA  v/s LDA
27-02-2021
### ANN,SVM Logistic
26-03-2021
#### Dataset- Credit Approval
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.graphics.gofplots import qqplot
import sklearn as sk
from datetime import datetime
from sklearn.model_selection import train_test_split
plt.style.use('ggplot')
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
from sklearn.preprocessing import MinMaxScaler
from sklearn.tree import DecisionTreeClassifier
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import accuracy_score
import mlxtend
from sklearn.metrics import plot_confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.decomposition import PCA as sklearnPCA
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix 
import tensorflow as tf

"""#### Collecting data"""

from google.colab import drive
drive.mount('/content/drive')
application = pd.read_excel('/content/drive/My Drive/application_record.xlsx')
application.head()
credit = pd.read_excel('/content/drive/My Drive/credit_record.xlsx')
credit.head()

"""#### Data Understanding and Data Pre-processing

###### Application record
* ID: Unique Id of the row
* CODE_GENDER: Gender of the applicant. M is male and F is female.
* FLAG_OWN_CAR: Is an applicant with a car. Y is Yes and N is NO.
* FLAG_OWN_REALTY: Is an applicant with realty. Y is Yes and N is No.
* CNT_CHILDREN: Count of children.
* AMT_INCOME_TOTAL: the amount of the income.
* NAME_INCOME_TYPE: The type of income (5 types in total).
* NAME_EDUCATION_TYPE: The type of education (5 types in total).
* NAME_FAMILY_STATUS: The type of family status (6 types in total).
* DAYS_BIRTH: The number of the days from birth (Negative values).
* DAYS_EMPLOYED: The number of the days from employed (Negative values).
* FLAG_MOBIL: Is an applicant with a mobile. 1 is True and 0 is False.
* FLAG_WORK_PHONE: Is an applicant with a work phone. 1 is True and 0 is False.
* FLAG_PHONE: Is an applicant with a phone. 1 is True and 0 is False.
* FLAG_EMAIL: Is an applicant with a email. 1 is True and 0 is False.
* OCCUPATION_TYPE: The type of occupation (19 types in total). This column has missing values.
* CNT_FAM_MEMBERS: The count of family members.
"""

application.head(4)

application.shape

#Checking for inconsistent data types
application.info()

#Finding missing values
application.isnull().sum()

del application['OCCUPATION_TYPE']

#Check for duplicate records
application['ID'].nunique() # the total rows are 438,557. This means it has duplicates

application = application.drop_duplicates('ID', keep='last') 
# we identified that there are some duplicates in this dataset
# we will be deleting those duplicates and will keep the last entry of the ID if its repeated.

"""##### Data Viz"""

#Count of children
fig = plt.figure(figsize=(7,4))
sns.countplot(x="CNT_CHILDREN",data=application,linewidth=2,edgecolor=sns.color_palette("dark", 1))
CNT_CHILDREN = application['CNT_CHILDREN'].value_counts()
for a,b in zip(range(len(CNT_CHILDREN)), CNT_CHILDREN):
    plt.text(a, b+50, '%.0f' % b, ha='center', va= 'bottom',fontsize=14)
plt.show()

#Count of family members
fig = plt.figure(figsize=(7,4))
sns.countplot(x="CNT_FAM_MEMBERS", data=application, palette="Greens_d")
CNT_FAM_MEMBERS = application.CNT_FAM_MEMBERS.apply(int).value_counts().sort_index()
for a,b in zip(range(len(CNT_FAM_MEMBERS)), CNT_FAM_MEMBERS):
    plt.text(a, b+50, '%.0f' % b, ha='center', va= 'bottom',fontsize=12)

plt.show()

#Distribution of Total income
fig = plt.figure(figsize=(5,4))
sns.distplot(application['AMT_INCOME_TOTAL'])
plt.title('Income Distribution')
plt.xlabel('Income')
plt.ylabel('Frequency')
plt.show()

fig = plt.figure(figsize=(5,4))
age = application.DAYS_BIRTH.apply(lambda x: int(-x / 365.25))
age_plot = pd.Series(age, name="age")
sns.distplot(age_plot)
plt.show()

#The number of the days from employed
fig = plt.figure(figsize=(5,4))
employed_year = application[application.DAYS_EMPLOYED<0].DAYS_EMPLOYED.apply(lambda x: int(-x // 365.25))
employed_plot = pd.Series(employed_year, name="employed_year")
sns.distplot(employed_plot)
plt.show()

#Gender proportion in applicants
gender_val = application.CODE_GENDER.value_counts(normalize = True)
gender_val
gender_val.plot.pie()
plt.show()

"""Around 67.14% of the applicants are female"""

#Applicants with cars
fig = plt.figure(figsize=(3,3))
x = application['FLAG_OWN_CAR'].value_counts()
sns.barplot(x.index,x)

#Applicant realty
fig = plt.figure(figsize=(3,3))
x = application['FLAG_OWN_REALTY'].value_counts()
sns.barplot(x.index,x)

#Type of income
fig = plt.figure(figsize=(9,3))
x = application['NAME_INCOME_TYPE'].value_counts()
sns.barplot(x.index,x)

#Level of education
fig = plt.figure(figsize=(12,3))
x = application['NAME_EDUCATION_TYPE'].value_counts()
sns.barplot(x.index,x)

#Education level and income relation
fig = plt.figure(figsize=(12,2))
application.groupby(["NAME_EDUCATION_TYPE"]).AMT_INCOME_TOTAL.mean().sort_values(ascending=False).plot.barh()
plt.show()

"""The average income increases with the education level."""

#Type of family status
fig = plt.figure(figsize=(12,2))
x = application['NAME_FAMILY_STATUS'].value_counts()
sns.barplot(x.index,x)

#Applicants' housing type
fig = plt.figure(figsize=(12,3))
x = application['NAME_HOUSING_TYPE'].value_counts()
sns.barplot(x.index,x)

#Outlier detection
d=application[['AMT_INCOME_TOTAL','DAYS_BIRTH','DAYS_EMPLOYED']]
d.plot(kind='box', subplots=True, sharex=False, sharey=False, figsize=(10,10), layout=(3,4))
plt.show()

#Outlier removal(not used,just checking)
print('original shape of dataset :',d.shape)
#criteria
Q1 = d.quantile(0.25)
Q3 = d.quantile(0.75)
IQR = Q3-Q1
max_ = Q3+1.5*IQR
min_ = Q1-1.5*IQR
#filter the outlier s
condition = (d <= max_) & (d >= min_)
condition = condition.all(axis=1)
df = d[condition]
print('filtered dataset shape : ',df.shape)
df.plot(kind='box', subplots=True, sharex=False, sharey=False, figsize=(10,10), layout=(3,4))
plt.show()

#Checking for normalization
fig = plt.figure(figsize=(6,3))
sns.distplot(application['AMT_INCOME_TOTAL'])
plt.title('Income Distribution')
plt.xlabel('Income')
plt.ylabel('Frequency')
plt.show()

fig = plt.figure(figsize=(6,1))
qqplot(application[['AMT_INCOME_TOTAL']], line='s')
plt.show()

#Outlier detection-Method2
Q1 = application[['AMT_INCOME_TOTAL','DAYS_EMPLOYED']].quantile(0.25)
Q3 = application[['AMT_INCOME_TOTAL','DAYS_EMPLOYED']] .quantile(0.75)
IQR = Q3-Q1
IQR

application['DAYS_EMPLOYED']=application[['DAYS_EMPLOYED']]*-1
application['DAYS_BIRTH']=application[['DAYS_BIRTH']]*-1
application.head()

#Normalizing
application[['AMT_INCOME_TOTAL','DAYS_EMPLOYED']]=sk.preprocessing.normalize(application[['AMT_INCOME_TOTAL','DAYS_EMPLOYED']], norm='l2', axis=0, copy=False, return_norm=False)
application.head(3)

#Checking for outliers
Q1 = application[['AMT_INCOME_TOTAL','DAYS_EMPLOYED']].quantile(0.25)
Q3 = application[['AMT_INCOME_TOTAL','DAYS_EMPLOYED']] .quantile(0.75)
IQR = Q3-Q1
IQR

"""##### Credit record
This is a csv file with credit record for a part of ID in application record. We can treat it a file to generate labels for modeling. For the applicants who have a record more than 59 past due, they should be rejected

* ID: Unique Id of the row in application record.
* MONTHS_BALANCE: The number of months from record time.
* STATUS: Credit status for this month.


  X: No loan for the month
  C: paid off that month 
  0: 1-29 days past due 
  1: 30-59 days past due 
  2: 60-89 days overdue
  3: 90-119 days overdue 
  4: 120-149 days overdue 
  5: Overdue or bad debts, write-offs for more than 150 days
"""

credit.head(3)

credit.shape

#Checking for inconsistent data types
credit.info()

#Check for duplicate records
credit['ID'].nunique() 
# this has around 45,000 unique rows as there are repeating entries for different monthly values and status.

#Finding missing values
credit.isnull().sum()

#Checking for outliers
fig = plt.figure(figsize=(3,2))
credit['MONTHS_BALANCE'].plot(kind='box')

#records matching in two datasets
len(set(credit['ID']).intersection(set(application['ID'])))

"""#### Data viz"""

fig = plt.figure(figsize=(7,4))
ax = fig.add_subplot()
ax.set_title('Correlation Plot', fontsize=18)
sns.heatmap(application[['CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'FLAG_WORK_PHONE', 
                         'FLAG_PHONE', 'FLAG_EMAIL', 'CNT_FAM_MEMBERS']].corr(), ax=ax)

"""There's a strong correlation between DAYS_EMPLOYED and DAYS_BIRTH, CNT_FAM_MEMBERS and CNT_CHIDREN"""

#filtering the columns that have non numeric values
obj = pd.DataFrame(application.dtypes =='object').reset_index()
object_type = obj[obj[0] == True]['index']
object_type

##filtering the columns that have numeric values
num_type = pd.DataFrame(application.dtypes != 'object').reset_index()
num_type = num_type[num_type[0] ==True]['index']
num_type

#LabelEncoding-transforming all the non numeric data columns into datacolumns of 0 and 1s
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
for x in application:
    
    if application[x].dtypes=='object':
        application[x] = le.fit_transform(application[x])

application.head(3)

#calculating months from today column to see how much old the month is 
credit['Months_from_today'] = credit['MONTHS_BALANCE']*-1
credit = credit.sort_values(['ID','Months_from_today'], ascending=True)
credit.head(3)

#Understanding the feature 'Status'
credit['STATUS'].value_counts()

#Status will be the label/prediction result for our model
#Replacing the value C and X with 0 as it is the same type and 1,2,3,4,5 are classified as 1 because they are the same type
credit['STATUS'].replace({'C': 0, 'X' : 0}, inplace=True)
credit['STATUS'] = credit['STATUS'].astype('int')
credit['STATUS'] = credit['STATUS'].apply(lambda x:1 if x >= 2 else 0)

#Normalizing the Status counts
credit['STATUS'].value_counts(normalize=True)

"""It can be see that the data is oversampled for the labels since '0' are 99% '1' are only 1% in the whole dataset.This oversampling issue needs to be addressed in order to make sense of the further analysis.This problem will be dealt with after combining both datasets."""

#Grouping the data in 'credit' dataset by 'ID' so that we can join it with 'application' dataset
credit_grp = credit.groupby('ID').agg(max).reset_index()
credit_grp.head(3)

# Combining the datasets
df = application.join(credit_grp.set_index('ID'), on='ID', how='inner')
df

df.info()

"""##### Data Viz"""

X = df.iloc[:,1:-1] # All the variables except labels
y = df.iloc[:,-2] # Labels

#Creating the test train split first
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)

#Fitting and transforming the data into a scaler for accurate result
mms = MinMaxScaler()
X_scaled = pd.DataFrame(mms.fit_transform(X_train), columns=X_train.columns)
X_test_scaled = pd.DataFrame(mms.transform(X_test), columns=X_test.columns)

from imblearn.over_sampling import SMOTE
oversample = SMOTE()
X_balanced, y_balanced = oversample.fit_resample(X_scaled, y_train)
X_test_balanced, y_test_balanced = oversample.fit_resample(X_test_scaled, y_test)
# we have addressed the issue of oversampling here

y_train.value_counts()

y_test.value_counts()

Method=['Decision Tree','Random Forest','KNN','SVM']
accuracy=[]

"""##### Decision Tree"""

model = DecisionTreeClassifier(max_depth=12,min_samples_split=8,random_state=1024)
model.fit(X_balanced, y_balanced)
y_predict = model.predict(X_test_balanced)

print('Accuracy Score is {:.4}'.format(accuracy_score(y_test_balanced, y_predict)))
print(pd.DataFrame(confusion_matrix(y_test_balanced,y_predict)))
class_names = ['0','1']
#plot_confusion_matrix(confusion_matrix(y_test_balanced,y_predict),classes=class_names, normalize = True, title='Normalized Confusion Matrix: CART')
titles_opt = [("Confusion matrix, without normalization", None),("Normalized confusion matrix", 'true')]
for title, normalize in titles_opt:
    disp = plot_confusion_matrix(model, X_test_balanced, y_test_balanced,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)
    disp.ax_.set_title(title)

print(title)
print(disp.confusion_matrix)   
plt.show()
result1 = classification_report(y_test_balanced, y_predict)
print("Classification Report:",)
print (result1)
accuracy.append(round(accuracy_score(y_test_balanced, y_predict)*100,2))

"""##### Random Forest"""

model = RandomForestClassifier(n_estimators=250,max_depth=12,min_samples_leaf=16)
model.fit(X_balanced, y_balanced)
y_predict = model.predict(X_test_balanced)

print('Accuracy Score is {:.4}'.format(accuracy_score(y_test_balanced, y_predict)))
print(pd.DataFrame(confusion_matrix(y_test_balanced,y_predict)))
class_names = ['0','1']
titles_opt = [("Confusion matrix, without normalization", None),("Normalized confusion matrix", 'true')]
for title, normalize in titles_opt:
    disp = plot_confusion_matrix(model, X_test_balanced, y_test_balanced,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)
    disp.ax_.set_title(title)

print(title)
print(disp.confusion_matrix)   
plt.show()
result1 = classification_report(y_test_balanced, y_predict)
print("Classification Report:",)
print (result1)
#accuracy.append(round(accuracy_score(y_test_balanced, y_predict)*100,2))

"""#### K-Nearest Neighbors (KNN)"""

model = KNeighborsClassifier(n_neighbors=7)
model.fit(X_balanced, y_balanced)
y_predict = model.predict(X_test_balanced)

print('Accuracy Score is {:.4}'.format(accuracy_score(y_test_balanced, y_predict)))
print(pd.DataFrame(confusion_matrix(y_test_balanced,y_predict)))
class_names = ['0','1']
titles_opt = [("Confusion matrix, without normalization", None),("Normalized confusion matrix", 'true')]
for title, normalize in titles_opt:
    disp = plot_confusion_matrix(model, X_test_balanced, y_test_balanced,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)
    disp.ax_.set_title(title)

print(title)
print(disp.confusion_matrix)   
plt.show()
result1 = classification_report(y_test_balanced, y_predict)
print("Classification Report:",)
print (result1)
accuracy.append(round(accuracy_score(y_test_balanced, y_predict)*100,2))

"""#### Support vector machines"""

model = SVC(C = 0.8,kernel='rbf')
model.fit(X_balanced, y_balanced)
y_predict = model.predict(X_test_balanced)

print('Accuracy Score is {:.4}'.format(accuracy_score(y_test_balanced, y_predict)))
print(pd.DataFrame(confusion_matrix(y_test_balanced,y_predict)))
class_names = ['0','1']
titles_opt = [("Confusion matrix, without normalization", None),("Normalized confusion matrix", 'true')]
for title, normalize in titles_opt:
    disp = plot_confusion_matrix(model, X_test_balanced, y_test_balanced,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)
    disp.ax_.set_title(title)

print(title)
print(disp.confusion_matrix)   
plt.show()
result1 = classification_report(y_test_balanced, y_predict)
print("Classification Report:",)
print (result1)
accuracy.append(round(accuracy_score(y_test_balanced, y_predict)*100,2))

Accuracy=pd.DataFrame([Method,accuracy]).T
Accuracy.columns=['Algorithm','Accuracy_score']
Accuracy.sort_values(by='Accuracy_score',ascending=False)

"""# Lab 4
### PCA vs LDA

**PCA**

PCA has no concern with the class labels. In simple words, PCA summarizes the feature set
without relying on the output. PCA tries to nd the directions of the maximum variance in the
dataset. In a large feature set, there are many features that are merely duplicate of the other
features or have a high correlation with the other features. Such features are basically redundant
and can be ignored. The role of PCA is to nd such highly correlated or duplicate features and to
come up with a new feature set where there is minimum correlation between the features or in
other words feature set with maximum variance between the features. Since the variance
between the features doesn't depend upon the output, therefore PCA doesn't take the output
labels into account.
"""

X = df.iloc[:,1:-1] # All the variables except labels
y = df.iloc[:,-2] # Labels

#Creating the test train split first
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)

#Fitting and transforming the data into a scaler for accurate result
mms = MinMaxScaler()
X_scaled = pd.DataFrame(mms.fit_transform(X_train), columns=X_train.columns)
X_test_scaled = pd.DataFrame(mms.transform(X_test), columns=X_test.columns)

oversample = SMOTE()
X_balanced, y_balanced = oversample.fit_resample(X_scaled, y_train)
X_test_balanced, y_test_balanced = oversample.fit_resample(X_test_scaled, y_test)
# we have addressed the issue of oversampling here

y_train.value_counts()

y_test.value_counts()

"""**To get ~0.95 variance required components=11 **"""

pca=PCA(n_components=0.95)
pca.fit(X_balanced)
principalComponents = pca.fit_transform(X_balanced)
#print(pca.components_)

#Percentage of variance explained by each of the selected components
print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))

print("original shape:   ", X_balanced.shape)
print("transformed shape:",principalComponents.shape)

v=0
for i in pca.explained_variance_ratio_:
    v+=i
print(np.round((1-v)*100))

"""**It can be observed that the principal component 1 holds 18.81% of the information while the principal component 2 holds only 14.11% of the information, 3rd: 13.10 of the information abd so on. Also, while projecting 18-dimensional data to a 11-dimensional data, 4% information was lost.**

X_new = pca.inverse_transform(principalComponents)
plt.scatter(X_balanced.iloc[:, 0], X_balanced.iloc[:, 1], alpha=0.01,color='blue')
plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.02,marker='o',color='green')
plt.axis('equal');
"""

# Commented out IPython magic to ensure Python compatibility.
# % matplotlib inline
import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = (8,5)

fig, ax = plt.subplots()
xi = np.arange(1, 12, step=1)
y = np.cumsum(pca.explained_variance_ratio_)

plt.ylim(0.0,1.1)
plt.plot(xi, y, marker='o', linestyle='--', color='b')

plt.xlabel('Number of Components')
plt.xticks(np.arange(0, 19, step=1)) #change from 0-based array index to 1-based human-readable label
plt.ylabel('Cumulative variance (%)')
plt.title('The number of components needed to explain variance')

plt.axhline(y=0.95, color='r', linestyle='-')
plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)

ax.grid(axis='x')
plt.show()

"""**From the plot of explained variance v/s Number of components, it is found that for a 95% variance, 11 out of 18 components are required.**"""

sc = StandardScaler()
X_balanced= sc.fit_transform(X_balanced)
X_test_balanced = sc.transform(X_test_balanced)

pca = PCA(n_components=0.95)
pca.fit(X_balanced)
X_balanced1= pca.fit_transform(X_balanced)
X_test_balanced1 = pca.transform(X_test_balanced)

model = RandomForestClassifier(max_depth=2,random_state=0)
model.fit(X_balanced1, y_balanced)
y_predict = model.predict(X_test_balanced1)

"""'''print('Accuracy Score is {:.4}'.format(accuracy_score(y_test_balanced, y_predict)))
matrix = confusion_matrix(y_test_balanced,y_predict, labels=[1,0])
print('Confusion matrix : \n',matrix)
tp, fn, fp, tn = confusion_matrix(y_test_balanced,y_predict,labels=[1,0]).reshape(-1)
print('Outcome values : \n', tp, fn, fp, tn)
#classification report for precision, recall f1-score and accuracy
matrix = classification_report(y_test_balanced,y_predict,labels=[1,0])
print('Classification report : \n',matrix)''' 
"""

confusion = pd.crosstab(y_test_balanced, y_predict, rownames=['Actual'], colnames=['Predicted'], margins=True)
confusion

cm=confusion_matrix(y_test_balanced,y_predict) 
def accuracy(confusion_matrix):
    diagonal_sum = confusion_matrix.trace()
    sum_of_all_elements = confusion_matrix.sum()
    return diagonal_sum / sum_of_all_elements 
accuracy(cm)

print('Accuracy Score is {:.4}'.format(accuracy_score(y_test_balanced, y_predict)))
print(pd.DataFrame(confusion_matrix(y_test_balanced,y_predict)))
class_names = ['0','1']
titles_opt = [("Confusion matrix, without normalization", None),("Normalized confusion matrix", 'true')]
fig = plt.figure(figsize=(4,4))
for title, normalize in titles_opt:
   
    disp = plot_confusion_matrix(model, X_test_balanced1, y_test_balanced,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)
    disp.ax_.set_title(title)
plt.figure(figsize=(3,4))    
print(title)
print(disp.confusion_matrix)   
plt.figure(figsize=(3,4))
plt.show()
result1 = classification_report(y_test_balanced, y_predict)
print("Classification Report:",)
print (result1)

"""***To get ~0.95 variance required components=11 but then visualization gets impossible and contribution from later components is insignificant.So, we can drop those.Visualising the separation of classes (or clusters) is hard for data with more than 3 dimensions (features)***"""

#1 PCA components

pca_1= PCA(n_components=1)
pca_1.fit(X_balanced)
X_balanced11= pca_1.fit_transform(X_balanced)
X_test_balanced11 = pca_1.transform(X_test_balanced)

model1 = RandomForestClassifier(max_depth=2,random_state=0)
model1.fit(X_balanced11, y_balanced)
y_predict1 = model1.predict(X_test_balanced11)

cm_11=confusion_matrix(y_test_balanced,y_predict1) 
def accuracy(confusion_matrix):
    diagonal_sum = confusion_matrix.trace()
    sum_of_all_elements = confusion_matrix.sum()
    return diagonal_sum / sum_of_all_elements 
accuracy(cm_11)

#2 PCA components

pca2 = PCA(n_components=2)
pca2.fit(X_balanced)
X_balanced1_1= pca2.fit_transform(X_balanced)
X_test_balanced1_1 = pca2.transform(X_test_balanced)

model2 = RandomForestClassifier(max_depth=2,random_state=0)
model2.fit(X_balanced1_1, y_balanced)
y_predict2 = model2.predict(X_test_balanced1_1)

cm_2=confusion_matrix(y_test_balanced,y_predict2) 
def accuracy(confusion_matrix):
    diagonal_sum = confusion_matrix.trace()
    sum_of_all_elements = confusion_matrix.sum()
    return diagonal_sum / sum_of_all_elements 
accuracy(cm_2)

import plotly.express as px
vv = y_balanced.astype(str)
fig = px.scatter(X_balanced1_1, x=0, y=1, color=vv,width=600, height=400)
fig.show()

from matplotlib.colors import ListedColormap
X_set, y_set = X_test_balanced1_1, y_test_balanced
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, model2.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.55, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('RandomForest Classifier(Test set)')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.legend()
plt.show()

#PCA 3 components

pca3 = PCA(n_components=3)
pca3.fit(X_balanced)
X_balanced1_2= pca3.fit_transform(X_balanced)
X_test_balanced1_2= pca3.transform(X_test_balanced)

model3= RandomForestClassifier(max_depth=2,random_state=0)
model3.fit(X_balanced1_2, y_balanced)
y_predict3 = model3.predict(X_test_balanced1_2)

cm_2=confusion_matrix(y_test_balanced,y_predict3) 
def accuracy(confusion_matrix):
    diagonal_sum = confusion_matrix.trace()
    sum_of_all_elements = confusion_matrix.sum()
    return diagonal_sum / sum_of_all_elements 
accuracy(cm_2)

total_var = pca3.explained_variance_ratio_.sum() * 100
vv = y_balanced.astype(str)
fig = px.scatter_3d(X_balanced1_2, x=0, y=1, z=2, color=vv,
    title=f'Total Explained Variance: {total_var:.2f}%',
    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'})
fig.show()

"""**LDA**

LDA tries to reduce dimensions of the feature set while retaining the information that
discriminates output classes. LDA tries to nd a decision boundary around each cluster of a
class. It then projects the data points to new dimensions in a way that the clusters are as
separate from each other as possible and the individual elements within a cluster are as close to
the centroid of the cluster as possible. The new dimensions are ranked on the basis of their
ability to maximize the distance between the clusters and minimize the distance between the
data points within a cluster and their centroids. These new dimensions form the linear
discriminants of the feature set.
"""

sc = StandardScaler()
X_balanced= sc.fit_transform(X_balanced)
X_test_balanced = sc.transform(X_test_balanced)

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components=None) #None- takes all features
lda.fit(X_balanced,y_balanced)
lda_var_ratios = lda.explained_variance_ratio_
lda_var_ratios #all components are stored and the sum of explained variances is equal to 1.0

# Create a function
def select_n_components(var_ratio, goal_var: float) -> int:
    # Set initial variance explained so far
    total_variance = 0.0    
    # Set initial number of features
    n_components = 0    
    # For the explained variance of each feature:
    for explained_variance in var_ratio:        
        # Add the explained variance to the total
        total_variance += explained_variance        
        # Add one to the number of components
        n_components += 1        
        # If we reach our goal level of explained variance
        if total_variance >= goal_var:
            # End the loop
            break            
    # Return the number of components
    return n_components

select_n_components(lda_var_ratios, 0.95)

# Commented out IPython magic to ensure Python compatibility.
# % matplotlib inline
import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = (8,5)

fig, ax = plt.subplots()
xi = np.arange(1, 2, step=1)
y = np.cumsum(lda.explained_variance_ratio_)

plt.ylim(0.0,1.1)
plt.plot(xi, y, marker='o', linestyle='--', color='b')

plt.xlabel('Number of Components')
plt.xticks(np.arange(0, 20, step=1)) #change from 0-based array index to 1-based human-readable label
plt.ylabel('Cumulative variance (%)')
plt.title('The number of components needed to explain variance')

plt.axhline(y=0.95, color='r', linestyle='-')
plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)

ax.grid(axis='x')
plt.show()  #2 classes (0,1), yields maximally 1 column.

lda = LDA(n_components=1) # to check the performance of our classifier with a single linea
lda.fit(X_balanced,y_balanced)
X_balanced2 = lda.fit_transform(X_balanced, y_balanced)
X_test_balanced2 = lda.transform(X_test_balanced)
lda.explained_variance_ratio_

"""**The number of and sum of explained variances is equal to 1.0.This is because all the components are selected.**"""

model = RandomForestClassifier(max_depth=2,random_state=0)
model.fit(X_balanced2, y_balanced)
y_predict = model.predict(X_test_balanced2)

print('Accuracy Score is {:.4}'.format(accuracy_score(y_test_balanced, y_predict)))
print(pd.DataFrame(confusion_matrix(y_test_balanced,y_predict)))
class_names = ['0','1']
titles_opt = [("Confusion matrix, without normalization", None),("Normalized confusion matrix", 'true')]
for title, normalize in titles_opt:
   
    disp = plot_confusion_matrix(model, X_test_balanced2, y_test_balanced,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)
    disp.ax_.set_title(title)
   
print(title)
print(disp.confusion_matrix) 
plt.figure(figsize=(3,4)) 
plt.show()
result1 = classification_report(y_test_balanced, y_predict)
print("Classification Report:",)
print (result1)

cm2=confusion_matrix(y_test_balanced,y_predict)
def accuracy(confusion_matrix):
    diagonal_sum = confusion_matrix.trace()
    sum_of_all_elements = confusion_matrix.sum()
    return diagonal_sum / sum_of_all_elements 
accuracy(cm2)

pc1 = X_balanced2[:,0]
classs=y_balanced
g=pd.DataFrame(pc1,columns=['pc1'])
g['classs']=classs
sns.regplot(data = g, x = 'pc1',y = 'classs', fit_reg=False,scatter_kws = {'s':50})

"""** It was evident from the LDA implementation that only one top component also able to differentiate the class with great accuracy.**

**INFERENCES**


***PCA***
* For the PCA, to get ~0.95 variance, 11 components were required.
* It can be observed that the principal component 1 holds 18.81% of the information while the principal component 2 holds only 14.11% of the information, 3rd: 13.10 of the information abd so on. Also, while projecting 18-dimensional data to a 11-dimensional data, 4% information was lost.
* The model with 11 components gave an accuracy of 93.57%.
* To get ~0.95 variance required PCA components=11 but then visualization gets impossible and contribution from later components is insignificant.So, we can drop those.Visualising the separation of classes (or clusters) is hard for data with more than 3 dimensions (features)
* The accuracy increased with increase in the number of components.But considering just upto 3 PCA components did not have good enough variance and accuracy and further the loss of information was more.
* The accuracy of the model with 1 PCA component was 49.44%.
* The plots after implementation depicts the poor performance of PCA when 2 and 3 components were considered.

***LDA***
* n_components in LDA chosen should be lesser than  (n_classes - 1) for dimensionality reduction.
* There exists 2 classes (0,1). So, it yields maximally 1 LDA component.
* The number of and sum of explained variances is equal to 1.0.This is because all the components are selected.
* The LDA yielded an accuracy of 58.08%.
* From the last plot, it was evident from the LDA implementation from sracth too that only one top component also able to differentiate the class with great accuracy.

***PCA v/s LDA***
* PCA performs better in case where number of samples per class is less. Whereas LDA works better with large dataset having multiple classes; class separability is an important factor while reducing dimensionality. In our case, there were only 2 classes.This might be reason of LDA yielding just 59.17% accuracy. 
* However, the accuracy from model with 1 LDA component 59.17% was greater than that with 1 PCA component 51.43%. 
* It was evident from the LDA implementation  that only one top component also able to differentiate the class with great accuracy. Whereas, the plots after implementation depicts the poor performance of classification when PCA with 2 and 3 components were considered.
* **PCA when used with 11 components to get 0.95 variance helped in reducing the 'Curse of dimensionality'. Whereas, LDA performed good at classifying the STATUS variable(with just 1 component)**

## Lab5,6: ANN,SVM Logistic
"""

X = df.iloc[:,1:-1] # All the variables except labels
y = df.iloc[:,-2] # Labels

#Creating the test train split first
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)

#Fitting and transforming the data into a scaler for accurate result
mms = MinMaxScaler()
X_scaled = pd.DataFrame(mms.fit_transform(X_train), columns=X_train.columns)
X_test_scaled = pd.DataFrame(mms.transform(X_test), columns=X_test.columns)

oversample = SMOTE()
X_balanced, y_balanced = oversample.fit_resample(X_scaled, y_train)
X_test_balanced, y_test_balanced = oversample.fit_resample(X_test_scaled, y_test)
# we have addressed the issue of oversampling here

y_train.value_counts()

y_balanced.value_counts()

y_test.value_counts()

y_test_balanced .value_counts()

Method=['SVM','Logitic Regression','ANN']
accuracy=[]

"""#### Support Vector Machine

Support vector machines so called as SVM is a supervised learning algorithm which can be used for classification and regression problems.Kernels in SVM classification refer to the function that is responsible for defining the decision boundaries between the classes. Apart from the classic linear kernel which assumes that the different classes are separated by a straight line, a RBF (radial basis function) kernel is used when the boundaries are hypothesized to be curve-shaped.
"""

SVM_start_time = datetime.now()
model = SVC()
model.fit(X_balanced, y_balanced)
y_predict = model.predict(X_test_balanced)

print('Accuracy Score is {:.4}'.format(accuracy_score(y_test_balanced, y_predict)))
print(pd.DataFrame(confusion_matrix(y_test_balanced,y_predict)))
class_names = ['0','1']
titles_opt = [("Confusion matrix, without normalization", None),("Normalized confusion matrix", 'true')]
for title, normalize in titles_opt:
    disp = plot_confusion_matrix(model, X_test_balanced, y_test_balanced,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)
    disp.ax_.set_title(title)

print(title)
print(disp.confusion_matrix)   
plt.show()
result1 = classification_report(y_test_balanced, y_predict)
print("Classification Report:",)
print (result1)
accuracy.append(round(accuracy_score(y_test_balanced, y_predict)*100,2))
#Time complexity
svm_end_time = datetime.now()
SVM_timeComplexity = svm_end_time-SVM_start_time

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
# defining parameter range
param_grid = {'C': [0.1, 10],'gamma': [1,0.001],'kernel': ['rbf']}
grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)
# fitting the model for grid search
grid.fit(X_balanced, y_balanced)

# print best parameter after tuning
print(grid.best_params_)

# print how our model looks after hyper-parameter tuning
print(grid.best_estimator_)

grid_predictions = grid.predict(X_test_balanced)

# print classification report
print(classification_report(y_test_balanced, grid_predictions))

# Necessary imports: 
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn import metrics

# Perform 6-fold cross validation
scores = cross_val_score(model, df, y, cv=6)
print ("Cross-validated scores:", scores)

# Make cross validated predictions
predictions = cross_val_predict(model,df, y, cv=6)
accuracy = metrics.r2_score(y, predictions)
print ("Cross-Predicted Accuracy:", accuracy)

"""##### Advantages
* SVM can be used to solve both classification and regression problems. 
* SVM can efficiently handle non-linear data using Kernel trick.
* SVM is more effective in high dimensional spaces.
* SVM works relatively well when there is a clear margin of separation between classes.


##### Disadvantages 
* In cases where the number of features for each data point exceeds the number of training data samples, the SVM will underperform.
* SVM algorithm is not suitable for large data sets.
* Choosing an appropriate Kernel function (to handle the non-linear data) is not an easy task.In case of using a high dimension Kernel, you might generate too many support vectors which reduce the training speed drastically. 
* One must do feature scaling of variables before applying SVM.

### Logistic Regression
Logistic regression is a classification algorithm. It is used to predict a binary outcome based on a set of independent variables.Logistic regression models the probabilities for classification problems with two possible outcomes. It's an extension of the linear regression model for classification problems.(The coefficients (Beta values b) of the logistic regression algorithm must be estimated from your training data. This is done using maximum-likelihood estimation.)
"""

logReg_start_time = datetime.now()
model2=LogisticRegression()
model2.fit(X_balanced, y_balanced)
y_predict2 = model2.predict(X_test_balanced)

print('Accuracy Score is {:.4}'.format(accuracy_score(y_test_balanced, y_predict2)))
print(pd.DataFrame(confusion_matrix(y_test_balanced,y_predict2)))
class_names = ['0','1']
titles_opt = [("Confusion matrix, without normalization", None),("Normalized confusion matrix", 'true')]
for title, normalize in titles_opt:
    disp = plot_confusion_matrix(model2, X_test_balanced, y_test_balanced,display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)
    disp.ax_.set_title(title)

print(title)
print(disp.confusion_matrix)   
plt.show()
result2 = classification_report(y_test_balanced, y_predict2)
print("Classification Report:",)
print (result2)
accuracy.append(round(accuracy_score(y_test_balanced, y_predict2)*100,2))
logReg_end_time = datetime.now()
logReg_timeComplexity = logReg_end_time-logReg_start_time



"""##### Advantages
* Logistic Regression is one of the simplest machine learning algorithms and is easy to implement yet provides great training efficiency in some cases.
* In a low dimensional dataset having a sufficient number of training examples, logistic regression is less prone to over-fitting.
* Logistic Regression proves to be very efficient when the dataset has features that are linearly separable.
* Logistic Regression not only gives a measure of how relevant a predictor (coefficient size) is, but also its direction of association (positive or negative).
* it is very efficient, does not require too many computational resources, it’s highly interpretable, it doesn’t require input features to be scaled, it doesn’t require any tuning, it’s easy to regularize, and it outputs well-calibrated predicted probabilities.


#### Disadvantages 
* On high dimensional datasets, this may lead to the model being over-fit on the training set.
* Non linear problems can't be solved with logistic regression since it has a linear decision surface. Linearly separable data is rarely found in real world scenarios. 
* Logistic Regression requires moderate or no multicollinearity between independent variables.Repetition of information could lead to wrong training of parameters (weights) during minimizing the cost function.
* Only important and relevant features should be used to build a model otherwise the probabilistic predictions made by the model may be incorrect and the model's predictive value may degrade.
* This algorithm is sensitive to outliers.
* Logistic Regression requires a large dataset and also sufficient training examples for all the categories it needs to identify.
* Logistic Regression can only be used to predict discrete functions. Therefore, the dependent variable of Logistic Regression is restricted to the discrete number set.

### ANN
ANN is a computational model made up of simple,highly interconnected processing elements,which process info by their dynamic state response to external inputs.
"""

ANN_start_time = datetime.now()
sc = StandardScaler()
ann_train = sc.fit_transform(X_balanced)
ann_test = sc.fit_transform(X_test_balanced)
# Initializing the ANN
ann = tf.keras.models.Sequential()

# Adding the input layer and the first hidden layer
ann.add(tf.keras.layers.Dense(units=32, activation='relu'))

# Adding the second hidden layer
ann.add(tf.keras.layers.Dense(units=32, activation='relu'))

# Adding the output layer
ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

# Compiling the ANN
ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

# Training the ANN on the Training set
ann.fit(ann_train, y_balanced, batch_size = 32, epochs = 100)
ANN_end_time = datetime.now()
ANN_timeComplexity = ANN_end_time-ANN_start_time

# Making the Confusion Matrix
con_mat = confusion_matrix(y_test_balanced, ann_pred.round())
fig, ax = plt.subplots(figsize=(5,5))
sns.heatmap(con_mat, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel("Predicted Values")
plt.ylabel("True Values")
plt.show();

ann_pred = ann.predict(ann_test)
ann_pred = (ann_pred > 0.5)
accuracy.append(round(accuracy_score(y_test_balanced, ann_pred.round())*100,2))



Time = [SVM_timeComplexity,logReg_timeComplexity,ANN_timeComplexity]
compare = pd.DataFrame({'Methods': Method,
                       'Scores': accuracy,
                       'Time Complexity': Time})
compare.sort()